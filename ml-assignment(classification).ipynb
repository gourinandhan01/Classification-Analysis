{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d55741-b4fb-4a2c-bffa-37916d37a40d",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification Using Supervised Learning Techniques\n",
    "\n",
    "## Objective\n",
    "The objective of this assessment is to evaluate various supervised learning techniques on the breast cancer dataset and compare their performances.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Loading and Preprocessing \n",
    "### Steps:\n",
    "1. Load the dataset from `sklearn.datasets`.\n",
    "2. Handle any missing values.\n",
    "3. Perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f17fa3-f1b9-48ee-867f-f00aa613078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", X.isnull().sum())\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cba3b7-817d-4e99-9dda-fbe1d7b9a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression:\\n\", classification_report(y_test, y_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f51c7c-dbfb-4bb0-85e3-6042a90fe12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Classifier:\\n\", classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca95c39d-1935-441c-b792-b093ef2487aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Classifier:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec33202-167e-4832-a49b-652f07a00743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"Support Vector Machine:\\n\", classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b57de5a-8d2c-4695-8806-6a42a5b96670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(\"k-Nearest Neighbors:\\n\", classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09def0d-532c-45ac-8b7e-6f01ccfb9dcb",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "### How it Works:\n",
    "Logistic Regression is a linear model used for binary classification. It predicts the probability that a given input point belongs to a certain class, using the logistic function (sigmoid). The decision boundary is linear, meaning it works well when the classes are linearly separable.\n",
    "\n",
    "### Why Suitable:\n",
    "The breast cancer dataset is relatively simple and has clear class separations, making Logistic Regression a good fit. It is also computationally efficient and interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decision Tree Classifier\n",
    "### How it Works:\n",
    "A Decision Tree Classifier splits the data into subsets based on feature values, creating a tree structure. Each node in the tree represents a decision rule, and the leaves represent class labels. It recursively partitions the data to minimize impurity (like Gini impurity or entropy).\n",
    "\n",
    "### Why Suitable:\n",
    "Decision Trees handle both numerical and categorical data well and are interpretable. However, they are prone to overfitting, especially with high-dimensional data like the breast cancer dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Random Forest Classifier\n",
    "### How it Works:\n",
    "Random Forest is an ensemble method that constructs multiple decision trees during training. It then aggregates their predictions to improve accuracy and robustness. Each tree is trained on a random subset of the data, and features are randomly selected at each split.\n",
    "\n",
    "### Why Suitable:\n",
    "Random Forest is less prone to overfitting compared to a single decision tree, making it suitable for high-dimensional datasets like breast cancer data. It also performs well in handling feature interactions and complex relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Support Vector Machine (SVM)\n",
    "### How it Works:\n",
    "SVM finds the optimal hyperplane that best separates the classes in a high-dimensional feature space. It maximizes the margin between the closest points of each class, known as support vectors. The kernel trick allows SVM to work efficiently with non-linear data by mapping it to a higher-dimensional space.\n",
    "\n",
    "### Why Suitable:\n",
    "SVM is highly effective for high-dimensional data and can handle complex decision boundaries, making it a good choice for the breast cancer dataset, which has many features.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. k-Nearest Neighbors (k-NN)\n",
    "### How it Works:\n",
    "k-NN is a non-parametric, distance-based algorithm. It classifies a data point based on the majority class of its k nearest neighbors. The distance metric (usually Euclidean) is used to find the closest neighbors.\n",
    "\n",
    "### Why Suitable:\n",
    "k-NN is simple and works well with smaller datasets. However, it can struggle with high-dimensional data due to the \"curse of dimensionality,\" making it less suitable for this dataset without proper tuning and feature scaling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301b375-cff2-4b86-9399-9060527d7b6a",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ed4189-d7ad-4a5a-82be-06a76c1e06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy\n",
      "0  Logistic Regression  0.973684\n",
      "3                  SVM  0.973684\n",
      "2        Random Forest  0.964912\n",
      "1        Decision Tree  0.947368\n",
      "4                 k-NN  0.947368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'k-NN']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test, y_pred_log_reg),\n",
    "    accuracy_score(y_test, y_pred_dt),\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    accuracy_score(y_test, y_pred_svm),\n",
    "    accuracy_score(y_test, y_pred_knn)\n",
    "]\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({'Model': models, 'Accuracy': accuracies})\n",
    "print(results.sort_values(by='Accuracy', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71524d9c-824f-4d92-a289-c821abcfbe8e",
   "metadata": {},
   "source": [
    "\n",
    "### Best Performing Algorithm:\n",
    "- **Logistic Regression** and **SVM** both achieved the highest accuracy of **97.37%**. These models performed well because they can efficiently handle the linear separability of the dataset and deal with high-dimensional data.\n",
    "\n",
    "### Worst Performing Algorithm:\n",
    "- **Decision Tree** and **k-NN** both had the lowest accuracy of **94.74%**. While Decision Trees are interpretable, they are prone to overfitting, and k-NN suffers from the curse of dimensionality, especially when the dataset has many features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74b114-e41b-4491-abac-5ba5fecc1f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
